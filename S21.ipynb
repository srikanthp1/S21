{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTeLxVHepAiA",
        "outputId": "49f9c75d-be68-4c16-9e1e-963050d8c1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5058\n",
            "step 300: train loss 2.4197, val loss 2.4336\n",
            "step 400: train loss 2.3501, val loss 2.3562\n",
            "step 500: train loss 2.2963, val loss 2.3125\n",
            "step 600: train loss 2.2407, val loss 2.2496\n",
            "step 700: train loss 2.2054, val loss 2.2187\n",
            "step 800: train loss 2.1633, val loss 2.1866\n",
            "step 900: train loss 2.1241, val loss 2.1504\n",
            "step 1000: train loss 2.1036, val loss 2.1306\n",
            "step 1100: train loss 2.0698, val loss 2.1180\n",
            "step 1200: train loss 2.0380, val loss 2.0791\n",
            "step 1300: train loss 2.0248, val loss 2.0634\n",
            "step 1400: train loss 1.9926, val loss 2.0359\n",
            "step 1500: train loss 1.9697, val loss 2.0287\n",
            "step 1600: train loss 1.9627, val loss 2.0477\n",
            "step 1700: train loss 1.9403, val loss 2.0115\n",
            "step 1800: train loss 1.9090, val loss 1.9941\n",
            "step 1900: train loss 1.9092, val loss 1.9858\n",
            "step 2000: train loss 1.8847, val loss 1.9925\n",
            "step 2100: train loss 1.8724, val loss 1.9757\n",
            "step 2200: train loss 1.8580, val loss 1.9594\n",
            "step 2300: train loss 1.8560, val loss 1.9537\n",
            "step 2400: train loss 1.8412, val loss 1.9427\n",
            "step 2500: train loss 1.8141, val loss 1.9402\n",
            "step 2600: train loss 1.8292, val loss 1.9397\n",
            "step 2700: train loss 1.8116, val loss 1.9322\n",
            "step 2800: train loss 1.8032, val loss 1.9218\n",
            "step 2900: train loss 1.8022, val loss 1.9285\n",
            "step 3000: train loss 1.7955, val loss 1.9195\n",
            "step 3100: train loss 1.7672, val loss 1.9192\n",
            "step 3200: train loss 1.7568, val loss 1.9138\n",
            "step 3300: train loss 1.7551, val loss 1.9059\n",
            "step 3400: train loss 1.7549, val loss 1.8945\n",
            "step 3500: train loss 1.7383, val loss 1.8956\n",
            "step 3600: train loss 1.7242, val loss 1.8868\n",
            "step 3700: train loss 1.7273, val loss 1.8822\n",
            "step 3800: train loss 1.7176, val loss 1.8923\n",
            "step 3900: train loss 1.7219, val loss 1.8750\n",
            "step 4000: train loss 1.7131, val loss 1.8603\n",
            "step 4100: train loss 1.7105, val loss 1.8777\n",
            "step 4200: train loss 1.7033, val loss 1.8675\n",
            "step 4300: train loss 1.7038, val loss 1.8556\n",
            "step 4400: train loss 1.7057, val loss 1.8643\n",
            "step 4500: train loss 1.6875, val loss 1.8528\n",
            "step 4600: train loss 1.6887, val loss 1.8405\n",
            "step 4700: train loss 1.6834, val loss 1.8501\n",
            "step 4800: train loss 1.6675, val loss 1.8437\n",
            "step 4900: train loss 1.6684, val loss 1.8407\n",
            "step 5000: train loss 1.6643, val loss 1.8295\n",
            "step 5100: train loss 1.6711, val loss 1.8400\n",
            "step 5200: train loss 1.6582, val loss 1.8266\n",
            "step 5300: train loss 1.6648, val loss 1.8271\n",
            "step 5400: train loss 1.6588, val loss 1.8211\n",
            "step 5500: train loss 1.6460, val loss 1.7968\n",
            "step 5600: train loss 1.6628, val loss 1.8179\n",
            "step 5700: train loss 1.6559, val loss 1.8172\n",
            "step 5800: train loss 1.6392, val loss 1.8080\n",
            "step 5900: train loss 1.6465, val loss 1.8112\n",
            "step 6000: train loss 1.6373, val loss 1.8092\n",
            "step 6100: train loss 1.6356, val loss 1.7908\n",
            "step 6200: train loss 1.6433, val loss 1.7945\n",
            "step 6300: train loss 1.6288, val loss 1.8026\n",
            "step 6400: train loss 1.6265, val loss 1.8045\n",
            "step 6500: train loss 1.6225, val loss 1.7825\n",
            "step 6600: train loss 1.6215, val loss 1.7812\n",
            "step 6700: train loss 1.6219, val loss 1.8057\n",
            "step 6800: train loss 1.6236, val loss 1.7996\n",
            "step 6900: train loss 1.6167, val loss 1.7895\n",
            "step 7000: train loss 1.6131, val loss 1.7836\n",
            "step 7100: train loss 1.6193, val loss 1.7891\n",
            "step 7200: train loss 1.6114, val loss 1.8004\n",
            "step 7300: train loss 1.6100, val loss 1.7720\n",
            "step 7400: train loss 1.6086, val loss 1.7873\n",
            "step 7500: train loss 1.5967, val loss 1.7863\n",
            "step 7600: train loss 1.6076, val loss 1.7870\n",
            "step 7700: train loss 1.5913, val loss 1.7698\n",
            "step 7800: train loss 1.6056, val loss 1.7705\n",
            "step 7900: train loss 1.6038, val loss 1.7766\n",
            "step 8000: train loss 1.5987, val loss 1.7825\n",
            "step 8100: train loss 1.5988, val loss 1.7735\n",
            "step 8200: train loss 1.6035, val loss 1.7857\n",
            "step 8300: train loss 1.5934, val loss 1.7725\n",
            "step 8400: train loss 1.5938, val loss 1.7748\n",
            "step 8500: train loss 1.5933, val loss 1.7746\n",
            "step 8600: train loss 1.5902, val loss 1.7823\n",
            "step 8700: train loss 1.5865, val loss 1.7753\n",
            "step 8800: train loss 1.5716, val loss 1.7728\n",
            "step 8900: train loss 1.5892, val loss 1.7558\n",
            "step 9000: train loss 1.5836, val loss 1.7609\n",
            "step 9100: train loss 1.5718, val loss 1.7543\n",
            "step 9200: train loss 1.5849, val loss 1.7591\n",
            "step 9300: train loss 1.5708, val loss 1.7420\n",
            "step 9400: train loss 1.5808, val loss 1.7485\n",
            "step 9500: train loss 1.5766, val loss 1.7598\n",
            "step 9600: train loss 1.5742, val loss 1.7531\n",
            "step 9700: train loss 1.5720, val loss 1.7397\n",
            "step 9800: train loss 1.5616, val loss 1.7570\n",
            "step 9900: train loss 1.5703, val loss 1.7433\n",
            "step 10000: train loss 1.5670, val loss 1.7483\n",
            "step 10100: train loss 1.5663, val loss 1.7511\n",
            "step 10200: train loss 1.5581, val loss 1.7432\n",
            "step 10300: train loss 1.5577, val loss 1.7379\n",
            "step 10400: train loss 1.5661, val loss 1.7432\n",
            "step 10500: train loss 1.5651, val loss 1.7279\n",
            "step 10600: train loss 1.5673, val loss 1.7345\n",
            "step 10700: train loss 1.5589, val loss 1.7478\n",
            "step 10800: train loss 1.5563, val loss 1.7454\n",
            "step 10900: train loss 1.5467, val loss 1.7503\n",
            "step 11000: train loss 1.5486, val loss 1.7398\n",
            "step 11100: train loss 1.5447, val loss 1.7349\n",
            "step 11200: train loss 1.5549, val loss 1.7292\n",
            "step 11300: train loss 1.5555, val loss 1.7502\n",
            "step 11400: train loss 1.5535, val loss 1.7299\n",
            "step 11500: train loss 1.5499, val loss 1.7192\n",
            "step 11600: train loss 1.5530, val loss 1.7254\n",
            "step 11700: train loss 1.5413, val loss 1.7296\n",
            "step 11800: train loss 1.5385, val loss 1.7229\n",
            "step 11900: train loss 1.5472, val loss 1.7165\n",
            "step 12000: train loss 1.5333, val loss 1.7086\n",
            "step 12100: train loss 1.5320, val loss 1.7111\n",
            "step 12200: train loss 1.5411, val loss 1.7356\n",
            "step 12300: train loss 1.5346, val loss 1.7329\n",
            "step 12400: train loss 1.5511, val loss 1.7236\n",
            "step 12500: train loss 1.5475, val loss 1.7217\n",
            "step 12600: train loss 1.5426, val loss 1.7093\n",
            "step 12700: train loss 1.5502, val loss 1.7091\n",
            "step 12800: train loss 1.5438, val loss 1.7370\n",
            "step 12900: train loss 1.5409, val loss 1.7377\n",
            "step 13000: train loss 1.5366, val loss 1.7295\n",
            "step 13100: train loss 1.5356, val loss 1.7160\n",
            "step 13200: train loss 1.5380, val loss 1.7061\n",
            "step 13300: train loss 1.5327, val loss 1.7105\n",
            "step 13400: train loss 1.5357, val loss 1.7073\n",
            "step 13500: train loss 1.5265, val loss 1.7243\n",
            "step 13600: train loss 1.5335, val loss 1.7180\n",
            "step 13700: train loss 1.5248, val loss 1.7184\n",
            "step 13800: train loss 1.5295, val loss 1.7191\n",
            "step 13900: train loss 1.5313, val loss 1.7026\n",
            "step 14000: train loss 1.5230, val loss 1.7105\n",
            "step 14100: train loss 1.5258, val loss 1.7174\n",
            "step 14200: train loss 1.5278, val loss 1.7210\n",
            "step 14300: train loss 1.5228, val loss 1.7208\n",
            "step 14400: train loss 1.5134, val loss 1.6948\n",
            "step 14500: train loss 1.5243, val loss 1.6930\n",
            "step 14600: train loss 1.5280, val loss 1.7102\n",
            "step 14700: train loss 1.5328, val loss 1.7079\n",
            "step 14800: train loss 1.5334, val loss 1.7066\n",
            "step 14900: train loss 1.5279, val loss 1.7056\n",
            "step 15000: train loss 1.5210, val loss 1.7068\n",
            "step 15100: train loss 1.5263, val loss 1.7118\n",
            "step 15200: train loss 1.5197, val loss 1.6917\n",
            "step 15300: train loss 1.5160, val loss 1.6931\n",
            "step 15400: train loss 1.5263, val loss 1.6998\n",
            "step 15500: train loss 1.5163, val loss 1.6966\n",
            "step 15600: train loss 1.5265, val loss 1.6906\n",
            "step 15700: train loss 1.5196, val loss 1.6946\n",
            "step 15800: train loss 1.5208, val loss 1.7040\n",
            "step 15900: train loss 1.5168, val loss 1.7021\n",
            "step 16000: train loss 1.5104, val loss 1.7008\n",
            "step 16100: train loss 1.5116, val loss 1.6931\n",
            "step 16200: train loss 1.5116, val loss 1.7015\n",
            "step 16300: train loss 1.5022, val loss 1.6979\n",
            "step 16400: train loss 1.5177, val loss 1.6927\n",
            "step 16500: train loss 1.5087, val loss 1.7022\n",
            "step 16600: train loss 1.5083, val loss 1.6980\n",
            "step 16700: train loss 1.5055, val loss 1.6994\n",
            "step 16800: train loss 1.5090, val loss 1.7044\n",
            "step 16900: train loss 1.5143, val loss 1.7019\n",
            "step 17000: train loss 1.5173, val loss 1.7071\n",
            "step 17100: train loss 1.5124, val loss 1.7087\n",
            "step 17200: train loss 1.5204, val loss 1.7065\n",
            "step 17300: train loss 1.5078, val loss 1.6837\n",
            "step 17400: train loss 1.5140, val loss 1.6977\n",
            "step 17500: train loss 1.5102, val loss 1.6852\n",
            "step 17600: train loss 1.4988, val loss 1.6858\n",
            "step 17700: train loss 1.5037, val loss 1.7004\n",
            "step 17800: train loss 1.5067, val loss 1.6812\n",
            "step 17900: train loss 1.5127, val loss 1.7052\n",
            "step 18000: train loss 1.4939, val loss 1.6748\n",
            "step 18100: train loss 1.4937, val loss 1.6876\n",
            "step 18200: train loss 1.4997, val loss 1.7022\n",
            "step 18300: train loss 1.5026, val loss 1.6738\n",
            "step 18400: train loss 1.5055, val loss 1.6974\n",
            "step 18500: train loss 1.4991, val loss 1.6874\n",
            "step 18600: train loss 1.4947, val loss 1.6899\n",
            "step 18700: train loss 1.5059, val loss 1.7010\n",
            "step 18800: train loss 1.4985, val loss 1.6952\n",
            "step 18900: train loss 1.5007, val loss 1.6883\n",
            "step 19000: train loss 1.4968, val loss 1.6904\n",
            "step 19100: train loss 1.4935, val loss 1.6756\n",
            "step 19200: train loss 1.5083, val loss 1.6765\n",
            "step 19300: train loss 1.5020, val loss 1.6685\n",
            "step 19400: train loss 1.4957, val loss 1.6922\n",
            "step 19500: train loss 1.5005, val loss 1.6953\n",
            "step 19600: train loss 1.5010, val loss 1.6762\n",
            "step 19700: train loss 1.5006, val loss 1.6981\n",
            "step 19800: train loss 1.4961, val loss 1.6904\n",
            "step 19900: train loss 1.4922, val loss 1.6843\n",
            "step 20000: train loss 1.4791, val loss 1.6728\n",
            "step 20100: train loss 1.4854, val loss 1.6845\n",
            "step 20200: train loss 1.4954, val loss 1.6771\n",
            "step 20300: train loss 1.4870, val loss 1.6660\n",
            "step 20400: train loss 1.4917, val loss 1.6734\n",
            "step 20500: train loss 1.4953, val loss 1.6740\n",
            "step 20600: train loss 1.4962, val loss 1.6806\n",
            "step 20700: train loss 1.4808, val loss 1.6745\n",
            "step 20800: train loss 1.4903, val loss 1.6846\n",
            "step 20900: train loss 1.4895, val loss 1.6665\n",
            "step 21000: train loss 1.4875, val loss 1.6757\n",
            "step 21100: train loss 1.4939, val loss 1.6678\n",
            "step 21200: train loss 1.4881, val loss 1.6747\n",
            "step 21300: train loss 1.4937, val loss 1.7071\n",
            "step 21400: train loss 1.4757, val loss 1.6746\n",
            "step 21500: train loss 1.4902, val loss 1.6845\n",
            "step 21600: train loss 1.4875, val loss 1.6730\n",
            "step 21700: train loss 1.4794, val loss 1.6956\n",
            "step 21800: train loss 1.4789, val loss 1.6772\n",
            "step 21900: train loss 1.4827, val loss 1.6861\n",
            "step 22000: train loss 1.4837, val loss 1.6831\n",
            "step 22100: train loss 1.4861, val loss 1.7010\n",
            "step 22200: train loss 1.4815, val loss 1.6851\n",
            "step 22300: train loss 1.4830, val loss 1.6685\n",
            "step 22400: train loss 1.4859, val loss 1.6731\n",
            "step 22500: train loss 1.4879, val loss 1.6818\n",
            "step 22600: train loss 1.4835, val loss 1.6927\n",
            "step 22700: train loss 1.4867, val loss 1.6809\n",
            "step 22800: train loss 1.4846, val loss 1.6757\n",
            "step 22900: train loss 1.4756, val loss 1.6852\n",
            "step 23000: train loss 1.4831, val loss 1.6779\n",
            "step 23100: train loss 1.4849, val loss 1.6888\n",
            "step 23200: train loss 1.4656, val loss 1.6668\n",
            "step 23300: train loss 1.4830, val loss 1.6737\n",
            "step 23400: train loss 1.4798, val loss 1.6654\n",
            "step 23500: train loss 1.4785, val loss 1.6855\n",
            "step 23600: train loss 1.4846, val loss 1.6790\n",
            "step 23700: train loss 1.4793, val loss 1.6871\n",
            "step 23800: train loss 1.4661, val loss 1.6843\n",
            "step 23900: train loss 1.4800, val loss 1.6805\n",
            "step 24000: train loss 1.4728, val loss 1.6700\n",
            "step 24100: train loss 1.4780, val loss 1.6781\n",
            "step 24200: train loss 1.4703, val loss 1.6750\n",
            "step 24300: train loss 1.4692, val loss 1.6830\n",
            "step 24400: train loss 1.4783, val loss 1.6796\n",
            "step 24500: train loss 1.4698, val loss 1.6770\n",
            "step 24600: train loss 1.4811, val loss 1.6806\n",
            "step 24700: train loss 1.4632, val loss 1.6887\n",
            "step 24800: train loss 1.4679, val loss 1.6600\n",
            "step 24900: train loss 1.4714, val loss 1.6835\n",
            "step 25000: train loss 1.4694, val loss 1.6766\n",
            "step 25100: train loss 1.4763, val loss 1.6665\n",
            "step 25200: train loss 1.4654, val loss 1.6766\n",
            "step 25300: train loss 1.4619, val loss 1.6839\n",
            "step 25400: train loss 1.4720, val loss 1.6776\n",
            "step 25500: train loss 1.4709, val loss 1.6855\n",
            "step 25600: train loss 1.4715, val loss 1.6717\n",
            "step 25700: train loss 1.4657, val loss 1.6827\n",
            "step 25800: train loss 1.4752, val loss 1.6773\n",
            "step 25900: train loss 1.4502, val loss 1.6591\n",
            "step 26000: train loss 1.4640, val loss 1.6668\n",
            "step 26100: train loss 1.4768, val loss 1.6689\n",
            "step 26200: train loss 1.4587, val loss 1.6747\n",
            "step 26300: train loss 1.4693, val loss 1.6764\n",
            "step 26400: train loss 1.4750, val loss 1.6840\n",
            "step 26500: train loss 1.4701, val loss 1.6543\n",
            "step 26600: train loss 1.4667, val loss 1.6669\n",
            "step 26700: train loss 1.4615, val loss 1.6668\n",
            "step 26800: train loss 1.4605, val loss 1.6508\n",
            "step 26900: train loss 1.4604, val loss 1.6643\n",
            "step 27000: train loss 1.4643, val loss 1.6624\n",
            "step 27100: train loss 1.4607, val loss 1.6505\n",
            "step 27200: train loss 1.4674, val loss 1.6497\n",
            "step 27300: train loss 1.4674, val loss 1.6730\n",
            "step 27400: train loss 1.4765, val loss 1.6682\n",
            "step 27500: train loss 1.4744, val loss 1.6499\n",
            "step 27600: train loss 1.4602, val loss 1.6548\n",
            "step 27700: train loss 1.4595, val loss 1.6552\n",
            "step 27800: train loss 1.4562, val loss 1.6485\n",
            "step 27900: train loss 1.4569, val loss 1.6674\n",
            "step 28000: train loss 1.4496, val loss 1.6582\n",
            "step 28100: train loss 1.4772, val loss 1.6470\n",
            "step 28200: train loss 1.4641, val loss 1.6529\n",
            "step 28300: train loss 1.4604, val loss 1.6613\n",
            "step 28400: train loss 1.4579, val loss 1.6654\n",
            "step 28500: train loss 1.4541, val loss 1.6573\n",
            "step 28600: train loss 1.4634, val loss 1.6610\n",
            "step 28700: train loss 1.4517, val loss 1.6628\n",
            "step 28800: train loss 1.4570, val loss 1.6518\n",
            "step 28900: train loss 1.4592, val loss 1.6554\n",
            "step 29000: train loss 1.4647, val loss 1.6595\n",
            "step 29100: train loss 1.4563, val loss 1.6683\n",
            "step 29200: train loss 1.4583, val loss 1.6429\n",
            "step 29300: train loss 1.4537, val loss 1.6419\n",
            "step 29400: train loss 1.4604, val loss 1.6561\n",
            "step 29500: train loss 1.4587, val loss 1.6443\n",
            "step 29600: train loss 1.4558, val loss 1.6543\n",
            "step 29700: train loss 1.4588, val loss 1.6619\n",
            "step 29800: train loss 1.4670, val loss 1.6639\n",
            "step 29900: train loss 1.4611, val loss 1.6504\n",
            "step 30000: train loss 1.4533, val loss 1.6651\n",
            "step 30100: train loss 1.4539, val loss 1.6539\n",
            "step 30200: train loss 1.4472, val loss 1.6541\n",
            "step 30300: train loss 1.4623, val loss 1.6678\n",
            "step 30400: train loss 1.4647, val loss 1.6715\n",
            "step 30500: train loss 1.4685, val loss 1.6653\n",
            "step 30600: train loss 1.4646, val loss 1.6659\n",
            "step 30700: train loss 1.4500, val loss 1.6481\n",
            "step 30800: train loss 1.4557, val loss 1.6532\n",
            "step 30900: train loss 1.4557, val loss 1.6550\n",
            "step 31000: train loss 1.4482, val loss 1.6545\n",
            "step 31100: train loss 1.4468, val loss 1.6662\n",
            "step 31200: train loss 1.4553, val loss 1.6578\n",
            "step 31300: train loss 1.4580, val loss 1.6571\n",
            "step 31400: train loss 1.4593, val loss 1.6573\n",
            "step 31500: train loss 1.4587, val loss 1.6644\n",
            "step 31600: train loss 1.4506, val loss 1.6688\n",
            "step 31700: train loss 1.4504, val loss 1.6639\n",
            "step 31800: train loss 1.4490, val loss 1.6639\n",
            "step 31900: train loss 1.4489, val loss 1.6559\n",
            "step 32000: train loss 1.4372, val loss 1.6605\n",
            "step 32100: train loss 1.4518, val loss 1.6621\n",
            "step 32200: train loss 1.4529, val loss 1.6644\n",
            "step 32300: train loss 1.4567, val loss 1.6654\n",
            "step 32400: train loss 1.4450, val loss 1.6552\n",
            "step 32500: train loss 1.4464, val loss 1.6632\n",
            "step 32600: train loss 1.4536, val loss 1.6490\n",
            "step 32700: train loss 1.4588, val loss 1.6575\n",
            "step 32800: train loss 1.4485, val loss 1.6572\n",
            "step 32900: train loss 1.4467, val loss 1.6583\n",
            "step 33000: train loss 1.4518, val loss 1.6467\n",
            "step 33100: train loss 1.4523, val loss 1.6407\n",
            "step 33200: train loss 1.4619, val loss 1.6590\n",
            "step 33300: train loss 1.4628, val loss 1.6457\n",
            "step 33400: train loss 1.4605, val loss 1.6647\n",
            "step 33500: train loss 1.4568, val loss 1.6559\n",
            "step 33600: train loss 1.4395, val loss 1.6490\n",
            "step 33700: train loss 1.4453, val loss 1.6476\n",
            "step 33800: train loss 1.4528, val loss 1.6578\n",
            "step 33900: train loss 1.4447, val loss 1.6478\n",
            "step 34000: train loss 1.4482, val loss 1.6558\n",
            "step 34100: train loss 1.4472, val loss 1.6517\n",
            "step 34200: train loss 1.4430, val loss 1.6534\n",
            "step 34300: train loss 1.4453, val loss 1.6585\n",
            "step 34400: train loss 1.4428, val loss 1.6560\n",
            "step 34500: train loss 1.4492, val loss 1.6452\n",
            "step 34600: train loss 1.4455, val loss 1.6625\n",
            "step 34700: train loss 1.4491, val loss 1.6522\n",
            "step 34800: train loss 1.4469, val loss 1.6635\n",
            "step 34900: train loss 1.4573, val loss 1.6572\n",
            "step 35000: train loss 1.4456, val loss 1.6380\n",
            "step 35100: train loss 1.4385, val loss 1.6462\n",
            "step 35200: train loss 1.4515, val loss 1.6457\n",
            "step 35300: train loss 1.4403, val loss 1.6612\n",
            "step 35400: train loss 1.4446, val loss 1.6541\n",
            "step 35500: train loss 1.4377, val loss 1.6486\n",
            "step 35600: train loss 1.4383, val loss 1.6550\n",
            "step 35700: train loss 1.4327, val loss 1.6649\n",
            "step 35800: train loss 1.4473, val loss 1.6401\n",
            "step 35900: train loss 1.4532, val loss 1.6501\n",
            "step 36000: train loss 1.4393, val loss 1.6481\n",
            "step 36100: train loss 1.4405, val loss 1.6577\n",
            "step 36200: train loss 1.4372, val loss 1.6404\n",
            "step 36300: train loss 1.4366, val loss 1.6458\n",
            "step 36400: train loss 1.4436, val loss 1.6487\n",
            "step 36500: train loss 1.4457, val loss 1.6390\n",
            "step 36600: train loss 1.4420, val loss 1.6398\n",
            "step 36700: train loss 1.4363, val loss 1.6402\n",
            "step 36800: train loss 1.4368, val loss 1.6542\n",
            "step 36900: train loss 1.4400, val loss 1.6331\n",
            "step 37000: train loss 1.4519, val loss 1.6499\n",
            "step 37100: train loss 1.4387, val loss 1.6549\n",
            "step 37200: train loss 1.4423, val loss 1.6465\n",
            "step 37300: train loss 1.4405, val loss 1.6565\n",
            "step 37400: train loss 1.4453, val loss 1.6486\n",
            "step 37500: train loss 1.4446, val loss 1.6564\n",
            "step 37600: train loss 1.4331, val loss 1.6464\n",
            "step 37700: train loss 1.4391, val loss 1.6385\n",
            "step 37800: train loss 1.4390, val loss 1.6829\n",
            "step 37900: train loss 1.4545, val loss 1.6552\n",
            "step 38000: train loss 1.4433, val loss 1.6515\n",
            "step 38100: train loss 1.4415, val loss 1.6490\n",
            "step 38200: train loss 1.4410, val loss 1.6545\n",
            "step 38300: train loss 1.4402, val loss 1.6330\n",
            "step 38400: train loss 1.4389, val loss 1.6416\n",
            "step 38500: train loss 1.4495, val loss 1.6511\n",
            "step 38600: train loss 1.4413, val loss 1.6448\n",
            "step 38700: train loss 1.4442, val loss 1.6318\n",
            "step 38800: train loss 1.4366, val loss 1.6353\n",
            "step 38900: train loss 1.4353, val loss 1.6364\n",
            "step 39000: train loss 1.4319, val loss 1.6345\n",
            "step 39100: train loss 1.4295, val loss 1.6402\n",
            "step 39200: train loss 1.4323, val loss 1.6515\n",
            "step 39300: train loss 1.4337, val loss 1.6395\n",
            "step 39400: train loss 1.4324, val loss 1.6452\n",
            "step 39500: train loss 1.4283, val loss 1.6471\n",
            "step 39600: train loss 1.4330, val loss 1.6549\n",
            "step 39700: train loss 1.4282, val loss 1.6445\n",
            "step 39800: train loss 1.4327, val loss 1.6319\n",
            "step 39900: train loss 1.4283, val loss 1.6468\n",
            "step 40000: train loss 1.4405, val loss 1.6273\n",
            "step 40100: train loss 1.4405, val loss 1.6430\n",
            "step 40200: train loss 1.4282, val loss 1.6511\n",
            "step 40300: train loss 1.4447, val loss 1.6451\n",
            "step 40400: train loss 1.4422, val loss 1.6474\n",
            "step 40500: train loss 1.4293, val loss 1.6417\n",
            "step 40600: train loss 1.4389, val loss 1.6494\n",
            "step 40700: train loss 1.4382, val loss 1.6511\n",
            "step 40800: train loss 1.4391, val loss 1.6350\n",
            "step 40900: train loss 1.4344, val loss 1.6439\n",
            "step 41000: train loss 1.4285, val loss 1.6447\n",
            "step 41100: train loss 1.4395, val loss 1.6330\n",
            "step 41200: train loss 1.4265, val loss 1.6296\n",
            "step 41300: train loss 1.4181, val loss 1.6424\n",
            "step 41400: train loss 1.4318, val loss 1.6490\n",
            "step 41500: train loss 1.4379, val loss 1.6455\n",
            "step 41600: train loss 1.4378, val loss 1.6363\n",
            "step 41700: train loss 1.4393, val loss 1.6416\n",
            "step 41800: train loss 1.4290, val loss 1.6490\n",
            "step 41900: train loss 1.4380, val loss 1.6397\n",
            "step 42000: train loss 1.4409, val loss 1.6456\n",
            "step 42100: train loss 1.4264, val loss 1.6414\n",
            "step 42200: train loss 1.4363, val loss 1.6357\n",
            "step 42300: train loss 1.4330, val loss 1.6501\n",
            "step 42400: train loss 1.4271, val loss 1.6395\n",
            "step 42500: train loss 1.4244, val loss 1.6188\n",
            "step 42600: train loss 1.4380, val loss 1.6414\n",
            "step 42700: train loss 1.4246, val loss 1.6532\n",
            "step 42800: train loss 1.4316, val loss 1.6449\n",
            "step 42900: train loss 1.4375, val loss 1.6521\n",
            "step 43000: train loss 1.4288, val loss 1.6618\n",
            "step 43100: train loss 1.4260, val loss 1.6424\n",
            "step 43200: train loss 1.4346, val loss 1.6319\n",
            "step 43300: train loss 1.4284, val loss 1.6424\n",
            "step 43400: train loss 1.4329, val loss 1.6418\n",
            "step 43500: train loss 1.4292, val loss 1.6341\n",
            "step 43600: train loss 1.4164, val loss 1.6238\n",
            "step 43700: train loss 1.4336, val loss 1.6434\n",
            "step 43800: train loss 1.4399, val loss 1.6364\n",
            "step 43900: train loss 1.4374, val loss 1.6420\n",
            "step 44000: train loss 1.4227, val loss 1.6500\n",
            "step 44100: train loss 1.4309, val loss 1.6227\n",
            "step 44200: train loss 1.4385, val loss 1.6347\n",
            "step 44300: train loss 1.4373, val loss 1.6529\n",
            "step 44400: train loss 1.4261, val loss 1.6291\n",
            "step 44500: train loss 1.4153, val loss 1.6057\n",
            "step 44600: train loss 1.4184, val loss 1.6290\n",
            "step 44700: train loss 1.4336, val loss 1.6405\n",
            "step 44800: train loss 1.4330, val loss 1.6345\n",
            "step 44900: train loss 1.4258, val loss 1.6257\n",
            "step 45000: train loss 1.4227, val loss 1.6311\n",
            "step 45100: train loss 1.4308, val loss 1.6368\n",
            "step 45200: train loss 1.4350, val loss 1.6537\n",
            "step 45300: train loss 1.4279, val loss 1.6403\n",
            "step 45400: train loss 1.4224, val loss 1.6403\n",
            "step 45500: train loss 1.4203, val loss 1.6402\n",
            "step 45600: train loss 1.4296, val loss 1.6454\n",
            "step 45700: train loss 1.4307, val loss 1.6382\n",
            "step 45800: train loss 1.4246, val loss 1.6432\n",
            "step 45900: train loss 1.4252, val loss 1.6429\n",
            "step 46000: train loss 1.4265, val loss 1.6442\n",
            "step 46100: train loss 1.4277, val loss 1.6344\n",
            "step 46200: train loss 1.4288, val loss 1.6362\n",
            "step 46300: train loss 1.4253, val loss 1.6323\n",
            "step 46400: train loss 1.4313, val loss 1.6522\n",
            "step 46500: train loss 1.4268, val loss 1.6474\n",
            "step 46600: train loss 1.4332, val loss 1.6444\n",
            "step 46700: train loss 1.4163, val loss 1.6440\n",
            "step 46800: train loss 1.4255, val loss 1.6218\n",
            "step 46900: train loss 1.4260, val loss 1.6389\n",
            "step 47000: train loss 1.4190, val loss 1.6393\n",
            "step 47100: train loss 1.4272, val loss 1.6391\n",
            "step 47200: train loss 1.4357, val loss 1.6448\n",
            "step 47300: train loss 1.4222, val loss 1.6361\n",
            "step 47400: train loss 1.4313, val loss 1.6431\n",
            "step 47500: train loss 1.4259, val loss 1.6429\n",
            "step 47600: train loss 1.4254, val loss 1.6355\n",
            "step 47700: train loss 1.4201, val loss 1.6439\n",
            "step 47800: train loss 1.4198, val loss 1.6442\n",
            "step 47900: train loss 1.4343, val loss 1.6502\n",
            "step 48000: train loss 1.4260, val loss 1.6475\n",
            "step 48100: train loss 1.4308, val loss 1.6250\n",
            "step 48200: train loss 1.4293, val loss 1.6504\n",
            "step 48300: train loss 1.4187, val loss 1.6306\n",
            "step 48400: train loss 1.4172, val loss 1.6350\n",
            "step 48500: train loss 1.4212, val loss 1.6351\n",
            "step 48600: train loss 1.4233, val loss 1.6356\n",
            "step 48700: train loss 1.4138, val loss 1.6313\n",
            "step 48800: train loss 1.4266, val loss 1.6367\n",
            "step 48900: train loss 1.4236, val loss 1.6451\n",
            "step 49000: train loss 1.4201, val loss 1.6492\n",
            "step 49100: train loss 1.4266, val loss 1.6361\n",
            "step 49200: train loss 1.4197, val loss 1.6583\n",
            "step 49300: train loss 1.4277, val loss 1.6495\n",
            "step 49400: train loss 1.4212, val loss 1.6304\n",
            "step 49500: train loss 1.4226, val loss 1.6267\n",
            "step 49600: train loss 1.4185, val loss 1.6541\n",
            "step 49700: train loss 1.4126, val loss 1.6493\n",
            "step 49800: train loss 1.4162, val loss 1.6311\n",
            "step 49900: train loss 1.4094, val loss 1.6268\n",
            "step 50000: train loss 1.4207, val loss 1.6457\n",
            "step 50100: train loss 1.4238, val loss 1.6374\n",
            "step 50200: train loss 1.4155, val loss 1.6276\n",
            "step 50300: train loss 1.4189, val loss 1.6297\n",
            "step 50400: train loss 1.4199, val loss 1.6496\n",
            "step 50500: train loss 1.4212, val loss 1.6399\n",
            "step 50600: train loss 1.4252, val loss 1.6308\n",
            "step 50700: train loss 1.4207, val loss 1.6407\n",
            "step 50800: train loss 1.4187, val loss 1.6396\n",
            "step 50900: train loss 1.4236, val loss 1.6384\n",
            "step 51000: train loss 1.4182, val loss 1.6412\n",
            "step 51100: train loss 1.4168, val loss 1.6410\n",
            "step 51200: train loss 1.4155, val loss 1.6423\n",
            "step 51300: train loss 1.4213, val loss 1.6377\n",
            "step 51400: train loss 1.4169, val loss 1.6462\n",
            "step 51500: train loss 1.4144, val loss 1.6268\n",
            "step 51600: train loss 1.4206, val loss 1.6533\n",
            "step 51700: train loss 1.4267, val loss 1.6331\n",
            "step 51800: train loss 1.4230, val loss 1.6359\n",
            "step 51900: train loss 1.4203, val loss 1.6198\n",
            "step 52000: train loss 1.4198, val loss 1.6225\n",
            "step 52100: train loss 1.4230, val loss 1.6286\n",
            "step 52200: train loss 1.4157, val loss 1.6366\n",
            "step 52300: train loss 1.4239, val loss 1.6305\n",
            "step 52400: train loss 1.4234, val loss 1.6302\n",
            "step 52500: train loss 1.4151, val loss 1.6284\n",
            "step 52600: train loss 1.4153, val loss 1.6378\n",
            "step 52700: train loss 1.4107, val loss 1.6340\n",
            "step 52800: train loss 1.4181, val loss 1.6358\n",
            "step 52900: train loss 1.4168, val loss 1.6411\n",
            "step 53000: train loss 1.4109, val loss 1.6281\n",
            "step 53100: train loss 1.4214, val loss 1.6470\n",
            "step 53200: train loss 1.4139, val loss 1.6307\n",
            "step 53300: train loss 1.4137, val loss 1.6239\n",
            "step 53400: train loss 1.4207, val loss 1.6255\n",
            "step 53500: train loss 1.4177, val loss 1.6384\n",
            "step 53600: train loss 1.4070, val loss 1.6356\n",
            "step 53700: train loss 1.4105, val loss 1.6234\n",
            "step 53800: train loss 1.4127, val loss 1.6340\n",
            "step 53900: train loss 1.4314, val loss 1.6402\n",
            "step 54000: train loss 1.4045, val loss 1.6462\n",
            "step 54100: train loss 1.4146, val loss 1.6419\n",
            "step 54200: train loss 1.4078, val loss 1.6224\n",
            "step 54300: train loss 1.4122, val loss 1.6202\n",
            "step 54400: train loss 1.4198, val loss 1.6300\n",
            "step 54500: train loss 1.4146, val loss 1.6361\n",
            "step 54600: train loss 1.4124, val loss 1.6250\n",
            "step 54700: train loss 1.4139, val loss 1.6343\n",
            "step 54800: train loss 1.4127, val loss 1.6316\n",
            "step 54900: train loss 1.4143, val loss 1.6184\n",
            "step 55000: train loss 1.4086, val loss 1.6285\n",
            "step 55100: train loss 1.4105, val loss 1.6452\n",
            "step 55200: train loss 1.4140, val loss 1.6394\n",
            "step 55300: train loss 1.4125, val loss 1.6155\n",
            "step 55400: train loss 1.4185, val loss 1.6297\n",
            "step 55500: train loss 1.4089, val loss 1.6433\n",
            "step 55600: train loss 1.4307, val loss 1.6228\n",
            "step 55700: train loss 1.4184, val loss 1.6150\n",
            "step 55800: train loss 1.4148, val loss 1.6413\n",
            "step 55900: train loss 1.4079, val loss 1.6376\n",
            "step 56000: train loss 1.4121, val loss 1.6280\n",
            "step 56100: train loss 1.4111, val loss 1.6252\n",
            "step 56200: train loss 1.4136, val loss 1.6113\n",
            "step 56300: train loss 1.4190, val loss 1.6137\n",
            "step 56400: train loss 1.4162, val loss 1.6299\n",
            "step 56500: train loss 1.4203, val loss 1.6297\n",
            "step 56600: train loss 1.4059, val loss 1.6265\n",
            "step 56700: train loss 1.4018, val loss 1.6295\n",
            "step 56800: train loss 1.4090, val loss 1.6188\n",
            "step 56900: train loss 1.4122, val loss 1.6350\n",
            "step 57000: train loss 1.4098, val loss 1.6256\n",
            "step 57100: train loss 1.4084, val loss 1.6320\n",
            "step 57200: train loss 1.4091, val loss 1.6283\n",
            "step 57300: train loss 1.4076, val loss 1.6386\n",
            "step 57400: train loss 1.4052, val loss 1.6416\n",
            "step 57500: train loss 1.4147, val loss 1.6319\n",
            "step 57600: train loss 1.4112, val loss 1.6305\n",
            "step 57700: train loss 1.4149, val loss 1.6277\n",
            "step 57800: train loss 1.4038, val loss 1.6264\n",
            "step 57900: train loss 1.4091, val loss 1.6267\n",
            "step 58000: train loss 1.4092, val loss 1.6322\n",
            "step 58100: train loss 1.4162, val loss 1.6471\n",
            "step 58200: train loss 1.4124, val loss 1.6336\n",
            "step 58300: train loss 1.4196, val loss 1.6419\n",
            "step 58400: train loss 1.4083, val loss 1.6340\n",
            "step 58500: train loss 1.4086, val loss 1.6201\n",
            "step 58600: train loss 1.4110, val loss 1.6256\n",
            "step 58700: train loss 1.4115, val loss 1.6303\n",
            "step 58800: train loss 1.4080, val loss 1.6146\n",
            "step 58900: train loss 1.4115, val loss 1.6388\n",
            "step 59000: train loss 1.4073, val loss 1.6226\n",
            "step 59100: train loss 1.4020, val loss 1.6270\n",
            "step 59200: train loss 1.4167, val loss 1.6313\n",
            "step 59300: train loss 1.4061, val loss 1.6266\n",
            "step 59400: train loss 1.4160, val loss 1.6203\n",
            "step 59500: train loss 1.3997, val loss 1.6231\n",
            "step 59600: train loss 1.4221, val loss 1.6223\n",
            "step 59700: train loss 1.4187, val loss 1.6360\n",
            "step 59800: train loss 1.4028, val loss 1.6341\n",
            "step 59900: train loss 1.4104, val loss 1.6178\n",
            "step 60000: train loss 1.4121, val loss 1.6360\n",
            "step 60100: train loss 1.4069, val loss 1.6374\n",
            "step 60200: train loss 1.4043, val loss 1.6286\n",
            "step 60300: train loss 1.4006, val loss 1.6309\n",
            "step 60400: train loss 1.4028, val loss 1.6169\n",
            "step 60500: train loss 1.4133, val loss 1.6185\n",
            "step 60600: train loss 1.4027, val loss 1.6343\n",
            "step 60700: train loss 1.4051, val loss 1.6372\n",
            "step 60800: train loss 1.4207, val loss 1.6234\n",
            "step 60900: train loss 1.4053, val loss 1.6303\n",
            "step 61000: train loss 1.4062, val loss 1.6200\n",
            "step 61100: train loss 1.4036, val loss 1.6248\n",
            "step 61200: train loss 1.4045, val loss 1.6241\n",
            "step 61300: train loss 1.4078, val loss 1.6308\n",
            "step 61400: train loss 1.4090, val loss 1.6167\n",
            "step 61500: train loss 1.4131, val loss 1.6285\n",
            "step 61600: train loss 1.4114, val loss 1.6247\n",
            "step 61700: train loss 1.4025, val loss 1.6134\n",
            "step 61800: train loss 1.4065, val loss 1.6290\n",
            "step 61900: train loss 1.4051, val loss 1.6184\n",
            "step 62000: train loss 1.4120, val loss 1.6323\n",
            "step 62100: train loss 1.4186, val loss 1.6455\n",
            "step 62200: train loss 1.4056, val loss 1.6209\n",
            "step 62300: train loss 1.4144, val loss 1.6210\n",
            "step 62400: train loss 1.4137, val loss 1.6179\n",
            "step 62500: train loss 1.4156, val loss 1.6315\n",
            "step 62600: train loss 1.3981, val loss 1.6240\n",
            "step 62700: train loss 1.4126, val loss 1.6223\n",
            "step 62800: train loss 1.4045, val loss 1.6305\n",
            "step 62900: train loss 1.4036, val loss 1.6217\n",
            "step 63000: train loss 1.4078, val loss 1.6216\n",
            "step 63100: train loss 1.4036, val loss 1.6421\n",
            "step 63200: train loss 1.4075, val loss 1.6276\n",
            "step 63300: train loss 1.4056, val loss 1.6379\n",
            "step 63400: train loss 1.4097, val loss 1.6212\n",
            "step 63500: train loss 1.4112, val loss 1.6297\n",
            "step 63600: train loss 1.4019, val loss 1.6287\n",
            "step 63700: train loss 1.4126, val loss 1.6325\n",
            "step 63800: train loss 1.4052, val loss 1.6202\n",
            "step 63900: train loss 1.4103, val loss 1.6306\n",
            "step 64000: train loss 1.4105, val loss 1.6360\n",
            "step 64100: train loss 1.3969, val loss 1.6086\n",
            "step 64200: train loss 1.3967, val loss 1.6250\n",
            "step 64300: train loss 1.4088, val loss 1.6328\n",
            "step 64400: train loss 1.4052, val loss 1.6432\n",
            "step 64500: train loss 1.4043, val loss 1.6366\n",
            "step 64600: train loss 1.4096, val loss 1.6342\n",
            "step 64700: train loss 1.4067, val loss 1.6352\n",
            "step 64800: train loss 1.4112, val loss 1.6285\n",
            "step 64900: train loss 1.4031, val loss 1.6100\n",
            "step 65000: train loss 1.4057, val loss 1.6206\n",
            "step 65100: train loss 1.4012, val loss 1.6359\n",
            "step 65200: train loss 1.3987, val loss 1.6393\n",
            "step 65300: train loss 1.4095, val loss 1.6162\n",
            "step 65400: train loss 1.3979, val loss 1.6174\n",
            "step 65500: train loss 1.4086, val loss 1.6380\n",
            "step 65600: train loss 1.4061, val loss 1.6224\n",
            "step 65700: train loss 1.4036, val loss 1.6226\n",
            "step 65800: train loss 1.4063, val loss 1.6284\n",
            "step 65900: train loss 1.4072, val loss 1.6364\n",
            "step 66000: train loss 1.4104, val loss 1.6284\n",
            "step 66100: train loss 1.4080, val loss 1.6312\n",
            "step 66200: train loss 1.4051, val loss 1.6335\n",
            "step 66300: train loss 1.4069, val loss 1.6229\n",
            "step 66400: train loss 1.3987, val loss 1.6200\n",
            "step 66500: train loss 1.4053, val loss 1.6354\n",
            "step 66600: train loss 1.3999, val loss 1.6209\n",
            "step 66700: train loss 1.4145, val loss 1.6174\n",
            "step 66800: train loss 1.4123, val loss 1.6451\n",
            "step 66900: train loss 1.4049, val loss 1.6319\n",
            "step 67000: train loss 1.4008, val loss 1.6219\n",
            "step 67100: train loss 1.4047, val loss 1.6390\n",
            "step 67200: train loss 1.4048, val loss 1.6318\n",
            "step 67300: train loss 1.4111, val loss 1.6297\n",
            "step 67400: train loss 1.4114, val loss 1.6198\n",
            "step 67500: train loss 1.3987, val loss 1.6230\n",
            "step 67600: train loss 1.4010, val loss 1.6261\n",
            "step 67700: train loss 1.3918, val loss 1.6330\n",
            "step 67800: train loss 1.4046, val loss 1.6265\n",
            "step 67900: train loss 1.3981, val loss 1.6394\n",
            "step 68000: train loss 1.3937, val loss 1.6299\n",
            "step 68100: train loss 1.4036, val loss 1.6179\n",
            "step 68200: train loss 1.3999, val loss 1.6164\n",
            "step 68300: train loss 1.4045, val loss 1.6231\n",
            "step 68400: train loss 1.4041, val loss 1.6277\n",
            "step 68500: train loss 1.3953, val loss 1.6261\n",
            "step 68600: train loss 1.4013, val loss 1.6203\n",
            "step 68700: train loss 1.4068, val loss 1.6232\n",
            "step 68800: train loss 1.4049, val loss 1.6059\n",
            "step 68900: train loss 1.4068, val loss 1.6210\n",
            "step 69000: train loss 1.3975, val loss 1.6162\n",
            "step 69100: train loss 1.4010, val loss 1.6264\n",
            "step 69200: train loss 1.4039, val loss 1.6322\n",
            "step 69300: train loss 1.3920, val loss 1.6176\n",
            "step 69400: train loss 1.4046, val loss 1.6203\n",
            "step 69500: train loss 1.3972, val loss 1.6261\n",
            "step 69600: train loss 1.4052, val loss 1.6238\n",
            "step 69700: train loss 1.3985, val loss 1.6172\n",
            "step 69800: train loss 1.4022, val loss 1.6384\n",
            "step 69900: train loss 1.3913, val loss 1.6175\n",
            "step 70000: train loss 1.3924, val loss 1.6319\n",
            "step 70100: train loss 1.4052, val loss 1.6207\n",
            "step 70200: train loss 1.4023, val loss 1.6280\n",
            "step 70300: train loss 1.4025, val loss 1.6282\n",
            "step 70400: train loss 1.4015, val loss 1.6235\n",
            "step 70500: train loss 1.3964, val loss 1.6300\n",
            "step 70600: train loss 1.3991, val loss 1.6060\n",
            "step 70700: train loss 1.3968, val loss 1.6210\n",
            "step 70800: train loss 1.3944, val loss 1.6275\n",
            "step 70900: train loss 1.4088, val loss 1.6139\n",
            "step 71000: train loss 1.4017, val loss 1.6161\n",
            "step 71100: train loss 1.4011, val loss 1.6278\n",
            "step 71200: train loss 1.4004, val loss 1.6291\n",
            "step 71300: train loss 1.3921, val loss 1.6312\n",
            "step 71400: train loss 1.3983, val loss 1.6236\n",
            "step 71500: train loss 1.4072, val loss 1.6165\n",
            "step 71600: train loss 1.3965, val loss 1.6229\n",
            "step 71700: train loss 1.4011, val loss 1.6216\n",
            "step 71800: train loss 1.3934, val loss 1.6305\n",
            "step 71900: train loss 1.3987, val loss 1.6205\n",
            "step 72000: train loss 1.4071, val loss 1.6245\n",
            "step 72100: train loss 1.3999, val loss 1.6188\n",
            "step 72200: train loss 1.4060, val loss 1.6221\n",
            "step 72300: train loss 1.4014, val loss 1.6242\n",
            "step 72400: train loss 1.4059, val loss 1.6288\n",
            "step 72500: train loss 1.3999, val loss 1.6220\n",
            "step 72600: train loss 1.4069, val loss 1.6223\n",
            "step 72700: train loss 1.3899, val loss 1.6244\n",
            "step 72800: train loss 1.3971, val loss 1.6336\n",
            "step 72900: train loss 1.4010, val loss 1.6302\n",
            "step 73000: train loss 1.3956, val loss 1.6223\n",
            "step 73100: train loss 1.4005, val loss 1.6231\n",
            "step 73200: train loss 1.3980, val loss 1.6263\n",
            "step 73300: train loss 1.3973, val loss 1.6212\n",
            "step 73400: train loss 1.3978, val loss 1.6086\n",
            "step 73500: train loss 1.4005, val loss 1.6361\n",
            "step 73600: train loss 1.3906, val loss 1.6139\n",
            "step 73700: train loss 1.3953, val loss 1.6327\n",
            "step 73800: train loss 1.4026, val loss 1.6272\n",
            "step 73900: train loss 1.3979, val loss 1.6129\n",
            "step 74000: train loss 1.3984, val loss 1.6166\n",
            "step 74100: train loss 1.4017, val loss 1.6219\n",
            "step 74200: train loss 1.3968, val loss 1.6084\n",
            "step 74300: train loss 1.4044, val loss 1.6133\n",
            "step 74400: train loss 1.3928, val loss 1.6119\n",
            "step 74500: train loss 1.3989, val loss 1.6160\n",
            "step 74600: train loss 1.3927, val loss 1.6084\n",
            "step 74700: train loss 1.3954, val loss 1.6110\n",
            "step 74800: train loss 1.3901, val loss 1.6156\n",
            "step 74900: train loss 1.4167, val loss 1.6206\n",
            "step 75000: train loss 1.3928, val loss 1.6067\n",
            "step 75100: train loss 1.4067, val loss 1.6126\n",
            "step 75200: train loss 1.3932, val loss 1.6168\n",
            "step 75300: train loss 1.3938, val loss 1.6134\n",
            "step 75400: train loss 1.4019, val loss 1.6113\n",
            "step 75500: train loss 1.3905, val loss 1.6223\n",
            "step 75600: train loss 1.3965, val loss 1.6187\n",
            "step 75700: train loss 1.3974, val loss 1.6275\n",
            "step 75800: train loss 1.3917, val loss 1.6349\n",
            "step 75900: train loss 1.4010, val loss 1.6136\n",
            "step 76000: train loss 1.3952, val loss 1.6366\n",
            "step 76100: train loss 1.4053, val loss 1.6307\n",
            "step 76200: train loss 1.3981, val loss 1.6148\n",
            "step 76300: train loss 1.3954, val loss 1.6101\n",
            "step 76400: train loss 1.3946, val loss 1.6268\n",
            "step 76500: train loss 1.3970, val loss 1.6313\n",
            "step 76600: train loss 1.3981, val loss 1.6196\n",
            "step 76700: train loss 1.4035, val loss 1.6350\n",
            "step 76800: train loss 1.4047, val loss 1.6099\n",
            "step 76900: train loss 1.4008, val loss 1.6294\n",
            "step 77000: train loss 1.3947, val loss 1.6274\n",
            "step 77100: train loss 1.3888, val loss 1.6322\n",
            "step 77200: train loss 1.3930, val loss 1.6267\n",
            "step 77300: train loss 1.3905, val loss 1.6245\n",
            "step 77400: train loss 1.4043, val loss 1.6258\n",
            "step 77500: train loss 1.4002, val loss 1.6223\n",
            "step 77600: train loss 1.3986, val loss 1.6316\n",
            "step 77700: train loss 1.4004, val loss 1.6104\n",
            "step 77800: train loss 1.3884, val loss 1.6412\n",
            "step 77900: train loss 1.3916, val loss 1.6420\n",
            "step 78000: train loss 1.3938, val loss 1.6217\n",
            "step 78100: train loss 1.4012, val loss 1.6191\n",
            "step 78200: train loss 1.3908, val loss 1.6257\n",
            "step 78300: train loss 1.4015, val loss 1.6057\n",
            "step 78400: train loss 1.4077, val loss 1.6366\n",
            "step 78500: train loss 1.3856, val loss 1.6110\n",
            "step 78600: train loss 1.3952, val loss 1.6175\n",
            "step 78700: train loss 1.3981, val loss 1.6201\n",
            "step 78800: train loss 1.3914, val loss 1.6100\n",
            "step 78900: train loss 1.3994, val loss 1.6256\n",
            "step 79000: train loss 1.4045, val loss 1.6269\n",
            "step 79100: train loss 1.3898, val loss 1.6189\n",
            "step 79200: train loss 1.4036, val loss 1.6204\n",
            "step 79300: train loss 1.3870, val loss 1.6192\n",
            "step 79400: train loss 1.3988, val loss 1.6237\n",
            "step 79500: train loss 1.3938, val loss 1.6224\n",
            "step 79600: train loss 1.3874, val loss 1.6221\n",
            "step 79700: train loss 1.3924, val loss 1.6259\n",
            "step 79800: train loss 1.3869, val loss 1.6245\n",
            "step 79900: train loss 1.3918, val loss 1.6370\n",
            "step 80000: train loss 1.3953, val loss 1.6048\n",
            "step 80100: train loss 1.3920, val loss 1.6063\n",
            "step 80200: train loss 1.3906, val loss 1.6157\n",
            "step 80300: train loss 1.3915, val loss 1.6211\n",
            "step 80400: train loss 1.4005, val loss 1.6168\n",
            "step 80500: train loss 1.3872, val loss 1.6023\n",
            "step 80600: train loss 1.3994, val loss 1.6178\n",
            "step 80700: train loss 1.4028, val loss 1.6302\n",
            "step 80800: train loss 1.3904, val loss 1.6119\n",
            "step 80900: train loss 1.3979, val loss 1.6213\n",
            "step 81000: train loss 1.3969, val loss 1.6238\n",
            "step 81100: train loss 1.3957, val loss 1.6122\n",
            "step 81200: train loss 1.3897, val loss 1.6088\n",
            "step 81300: train loss 1.3885, val loss 1.6108\n",
            "step 81400: train loss 1.3923, val loss 1.5981\n",
            "step 81500: train loss 1.3918, val loss 1.6178\n",
            "step 81600: train loss 1.3941, val loss 1.6224\n",
            "step 81700: train loss 1.4016, val loss 1.6131\n",
            "step 81800: train loss 1.3833, val loss 1.6217\n",
            "step 81900: train loss 1.3897, val loss 1.6168\n",
            "step 82000: train loss 1.3921, val loss 1.6259\n",
            "step 82100: train loss 1.3868, val loss 1.6202\n",
            "step 82200: train loss 1.3913, val loss 1.6172\n",
            "step 82300: train loss 1.3975, val loss 1.6274\n",
            "step 82400: train loss 1.3933, val loss 1.6265\n",
            "step 82500: train loss 1.3958, val loss 1.6238\n",
            "step 82600: train loss 1.3959, val loss 1.6164\n",
            "step 82700: train loss 1.4005, val loss 1.6205\n",
            "step 82800: train loss 1.3869, val loss 1.6238\n",
            "step 82900: train loss 1.3896, val loss 1.6178\n",
            "step 83000: train loss 1.3918, val loss 1.6258\n",
            "step 83100: train loss 1.3900, val loss 1.6066\n",
            "step 83200: train loss 1.3891, val loss 1.6142\n",
            "step 83300: train loss 1.3955, val loss 1.6234\n",
            "step 83400: train loss 1.3804, val loss 1.6279\n",
            "step 83500: train loss 1.3836, val loss 1.6294\n",
            "step 83600: train loss 1.4014, val loss 1.6169\n",
            "step 83700: train loss 1.4029, val loss 1.6310\n",
            "step 83800: train loss 1.3865, val loss 1.6032\n",
            "step 83900: train loss 1.3953, val loss 1.6201\n",
            "step 84000: train loss 1.3835, val loss 1.6282\n",
            "step 84100: train loss 1.3914, val loss 1.6287\n",
            "step 84200: train loss 1.3922, val loss 1.6282\n",
            "step 84300: train loss 1.3937, val loss 1.6147\n",
            "step 84400: train loss 1.3869, val loss 1.6303\n",
            "step 84500: train loss 1.3949, val loss 1.6163\n",
            "step 84600: train loss 1.3907, val loss 1.6150\n",
            "step 84700: train loss 1.3901, val loss 1.6114\n",
            "step 84800: train loss 1.3908, val loss 1.6170\n",
            "step 84900: train loss 1.3945, val loss 1.6288\n",
            "step 85000: train loss 1.3907, val loss 1.6039\n",
            "step 85100: train loss 1.3874, val loss 1.6202\n",
            "step 85200: train loss 1.3892, val loss 1.6099\n",
            "step 85300: train loss 1.3860, val loss 1.6149\n",
            "step 85400: train loss 1.3884, val loss 1.6113\n",
            "step 85500: train loss 1.3905, val loss 1.6129\n",
            "step 85600: train loss 1.3877, val loss 1.6013\n",
            "step 85700: train loss 1.3887, val loss 1.6279\n",
            "step 85800: train loss 1.3796, val loss 1.6169\n",
            "step 85900: train loss 1.3829, val loss 1.6102\n",
            "step 86000: train loss 1.3876, val loss 1.6081\n",
            "step 86100: train loss 1.3885, val loss 1.6034\n",
            "step 86200: train loss 1.3869, val loss 1.6144\n",
            "step 86300: train loss 1.3882, val loss 1.6237\n",
            "step 86400: train loss 1.3922, val loss 1.6098\n",
            "step 86500: train loss 1.3964, val loss 1.6106\n",
            "step 86600: train loss 1.3996, val loss 1.6184\n",
            "step 86700: train loss 1.3865, val loss 1.6191\n",
            "step 86800: train loss 1.3813, val loss 1.6144\n",
            "step 86900: train loss 1.3773, val loss 1.6291\n",
            "step 87000: train loss 1.3871, val loss 1.6207\n",
            "step 87100: train loss 1.3938, val loss 1.6158\n",
            "step 87200: train loss 1.3950, val loss 1.6219\n",
            "step 87300: train loss 1.3934, val loss 1.6238\n",
            "step 87400: train loss 1.3825, val loss 1.6023\n",
            "step 87500: train loss 1.3855, val loss 1.6185\n",
            "step 87600: train loss 1.3909, val loss 1.6001\n",
            "step 87700: train loss 1.3895, val loss 1.6096\n",
            "step 87800: train loss 1.3935, val loss 1.6151\n",
            "step 87900: train loss 1.3870, val loss 1.6161\n",
            "step 88000: train loss 1.3894, val loss 1.6309\n",
            "step 88100: train loss 1.3880, val loss 1.5952\n",
            "step 88200: train loss 1.3921, val loss 1.6146\n",
            "step 88300: train loss 1.3886, val loss 1.6084\n",
            "step 88400: train loss 1.3791, val loss 1.6332\n",
            "step 88500: train loss 1.3873, val loss 1.6118\n",
            "step 88600: train loss 1.3916, val loss 1.6377\n",
            "step 88700: train loss 1.3841, val loss 1.6053\n",
            "step 88800: train loss 1.3859, val loss 1.6138\n",
            "step 88900: train loss 1.3815, val loss 1.6076\n",
            "step 89000: train loss 1.3823, val loss 1.6203\n",
            "step 89100: train loss 1.3973, val loss 1.5829\n",
            "step 89200: train loss 1.3901, val loss 1.5987\n",
            "step 89300: train loss 1.3948, val loss 1.6063\n",
            "step 89400: train loss 1.3865, val loss 1.6003\n",
            "step 89500: train loss 1.3791, val loss 1.6070\n",
            "step 89600: train loss 1.3954, val loss 1.6201\n",
            "step 89700: train loss 1.3959, val loss 1.6015\n",
            "step 89800: train loss 1.3852, val loss 1.6000\n",
            "step 89900: train loss 1.3836, val loss 1.6100\n",
            "step 90000: train loss 1.3871, val loss 1.6081\n",
            "step 90100: train loss 1.3822, val loss 1.6021\n",
            "step 90200: train loss 1.3885, val loss 1.6020\n",
            "step 90300: train loss 1.3773, val loss 1.6247\n",
            "step 90400: train loss 1.3949, val loss 1.6195\n",
            "step 90500: train loss 1.3866, val loss 1.6118\n",
            "step 90600: train loss 1.3883, val loss 1.6095\n",
            "step 90700: train loss 1.3853, val loss 1.6228\n",
            "step 90800: train loss 1.3919, val loss 1.6164\n",
            "step 90900: train loss 1.3934, val loss 1.6113\n",
            "step 91000: train loss 1.3890, val loss 1.6366\n",
            "step 91100: train loss 1.3831, val loss 1.6194\n",
            "step 91200: train loss 1.3897, val loss 1.6179\n",
            "step 91300: train loss 1.3812, val loss 1.6019\n",
            "step 91400: train loss 1.3817, val loss 1.6059\n",
            "step 91500: train loss 1.3972, val loss 1.6106\n",
            "step 91600: train loss 1.3870, val loss 1.6216\n",
            "step 91700: train loss 1.3848, val loss 1.6162\n",
            "step 91800: train loss 1.3839, val loss 1.6072\n",
            "step 91900: train loss 1.3853, val loss 1.6051\n",
            "step 92000: train loss 1.3814, val loss 1.5887\n",
            "step 92100: train loss 1.3904, val loss 1.5990\n",
            "step 92200: train loss 1.3902, val loss 1.5990\n",
            "step 92300: train loss 1.3858, val loss 1.5961\n",
            "step 92400: train loss 1.3843, val loss 1.6019\n",
            "step 92500: train loss 1.3889, val loss 1.6101\n",
            "step 92600: train loss 1.3861, val loss 1.6045\n",
            "step 92700: train loss 1.3879, val loss 1.6021\n",
            "step 92800: train loss 1.3931, val loss 1.6120\n",
            "step 92900: train loss 1.3863, val loss 1.6196\n",
            "step 93000: train loss 1.3863, val loss 1.6180\n",
            "step 93100: train loss 1.3900, val loss 1.6085\n",
            "step 93200: train loss 1.3809, val loss 1.5993\n",
            "step 93300: train loss 1.3888, val loss 1.6233\n",
            "step 93400: train loss 1.3860, val loss 1.5941\n",
            "step 93500: train loss 1.3787, val loss 1.6046\n",
            "step 93600: train loss 1.3839, val loss 1.6073\n",
            "step 93700: train loss 1.3879, val loss 1.6043\n",
            "step 93800: train loss 1.3859, val loss 1.6183\n",
            "step 93900: train loss 1.3766, val loss 1.6182\n",
            "step 94000: train loss 1.3878, val loss 1.6128\n",
            "step 94100: train loss 1.3776, val loss 1.6090\n",
            "step 94200: train loss 1.3916, val loss 1.6148\n",
            "step 94300: train loss 1.3798, val loss 1.6231\n",
            "step 94400: train loss 1.3827, val loss 1.6047\n",
            "step 94500: train loss 1.3855, val loss 1.6262\n",
            "step 94600: train loss 1.3948, val loss 1.6148\n",
            "step 94700: train loss 1.3860, val loss 1.6225\n",
            "step 94800: train loss 1.3963, val loss 1.6159\n",
            "step 94900: train loss 1.3838, val loss 1.6157\n",
            "step 95000: train loss 1.3913, val loss 1.6128\n",
            "step 95100: train loss 1.3769, val loss 1.6164\n",
            "step 95200: train loss 1.3786, val loss 1.6180\n",
            "step 95300: train loss 1.3831, val loss 1.6111\n",
            "step 95400: train loss 1.3782, val loss 1.6047\n",
            "step 95500: train loss 1.3843, val loss 1.6066\n",
            "step 95600: train loss 1.3789, val loss 1.6141\n",
            "step 95700: train loss 1.3840, val loss 1.6199\n",
            "step 95800: train loss 1.3711, val loss 1.6206\n",
            "step 95900: train loss 1.3876, val loss 1.6163\n",
            "step 96000: train loss 1.3779, val loss 1.6125\n",
            "step 96100: train loss 1.3915, val loss 1.6122\n",
            "step 96200: train loss 1.3871, val loss 1.6176\n",
            "step 96300: train loss 1.3885, val loss 1.6166\n",
            "step 96400: train loss 1.3920, val loss 1.6252\n",
            "step 96500: train loss 1.3854, val loss 1.6241\n",
            "step 96600: train loss 1.3870, val loss 1.6209\n",
            "step 96700: train loss 1.3774, val loss 1.6050\n",
            "step 96800: train loss 1.3807, val loss 1.6035\n",
            "step 96900: train loss 1.3905, val loss 1.6256\n",
            "step 97000: train loss 1.3765, val loss 1.5970\n",
            "step 97100: train loss 1.3770, val loss 1.6014\n",
            "step 97200: train loss 1.3945, val loss 1.6040\n",
            "step 97300: train loss 1.3897, val loss 1.6052\n",
            "step 97400: train loss 1.3799, val loss 1.6126\n",
            "step 97500: train loss 1.3844, val loss 1.6177\n",
            "step 97600: train loss 1.3826, val loss 1.6171\n",
            "step 97700: train loss 1.3832, val loss 1.6189\n",
            "step 97800: train loss 1.3858, val loss 1.6056\n",
            "step 97900: train loss 1.3806, val loss 1.6188\n",
            "step 98000: train loss 1.3830, val loss 1.6000\n",
            "step 98100: train loss 1.3824, val loss 1.6089\n",
            "step 98200: train loss 1.3889, val loss 1.5931\n",
            "step 98300: train loss 1.3783, val loss 1.6217\n",
            "step 98400: train loss 1.3876, val loss 1.6190\n",
            "step 98500: train loss 1.3728, val loss 1.6039\n",
            "step 98600: train loss 1.3750, val loss 1.6039\n",
            "step 98700: train loss 1.3799, val loss 1.5984\n",
            "step 98800: train loss 1.3809, val loss 1.6119\n",
            "step 98900: train loss 1.3828, val loss 1.5975\n",
            "step 99000: train loss 1.3871, val loss 1.6174\n",
            "step 99100: train loss 1.3854, val loss 1.6149\n",
            "step 99200: train loss 1.3809, val loss 1.5997\n",
            "step 99300: train loss 1.3860, val loss 1.6185\n",
            "step 99400: train loss 1.3876, val loss 1.6306\n",
            "step 99500: train loss 1.3797, val loss 1.6185\n",
            "step 99600: train loss 1.3797, val loss 1.6086\n",
            "step 99700: train loss 1.3706, val loss 1.6218\n",
            "step 99800: train loss 1.3793, val loss 1.6164\n",
            "step 99900: train loss 1.3792, val loss 1.6136\n",
            "step 99999: train loss 1.3862, val loss 1.6044\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "And cowr, Tyrrel, by made the Barnalle Sthould Murderer:\n",
            "I'll be genuary, here?\n",
            "\n",
            "Messengeress. Is my faces are zoloun\n",
            "Yours, to time I committed like\n",
            "es it end not will is thereve to thee flower only\n",
            "joys, liking peace thee, county son,\n",
            "I am said, yet losses nor\n",
            "To tell thee my loved is makes already her either with this.\n",
            "\n",
            "ESCALUS:\n",
            "Edward, my lord.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "I' the great whiteful flesh their dare?\n",
            "\n",
            "KING HENRY VI:\n",
            "Hark, by sad add by his great hoinous.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "After care, time not be, go.\n",
            "\n",
            "BISHOP OF CARLISLE:\n",
            "Those men would we met.\n",
            "\n",
            "CLROSDoOz:\n",
            "I have not us, I'll go to-days: your pecksou.\n",
            "\n",
            "ANGELO:\n",
            "Pray you, troy's pardon; alas, and part\n",
            "Less: sir, if thy seasons doubly\n",
            "Of Richard: there is grains?\n",
            "Whose is dam neest may chance bear\n",
            "to give them with some voicement;\n",
            "And stoples not a very creath. Fries, mink\n",
            "My shoons none Catcus, if I do it is determit.\n",
            "\n",
            "Keeper:\n",
            "Some right is better it, both speedly like to the hand!\n",
            "\n",
            "SLY:\n",
            "There is it, Warwick for Gloucester.\n",
            "Yet I am a rude heaven to the next of high conver,\n",
            "They hate to Englishace of him!\n",
            "\n",
            "ESCALUS:\n",
            "And not so leads\n",
            "Sinfined few I thee Warwick did chid not be\n",
            "Hear no morning tale time in may incertain\n",
            "In their will, tell thee eire breath it.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Save thou for Henry's mother; and there's direful men\n",
            "Did friends. Who's there's and be long ill.\n",
            "\n",
            "LARTIUS:\n",
            "A detinily quiverch the beguidence is not to be son.\n",
            "\n",
            "MISHOP MOWBRAY:\n",
            "I think, nurse, by meet, speed.\n",
            "\n",
            "LUCIO:\n",
            "He hath the gates: old the\n",
            "easy most of you; and pluck him every friend:\n",
            "Fake-latches Club!\n",
            "\n",
            "CAPULET:\n",
            "Nurse, Barit, sir?\n",
            "\n",
            "ESCALUS:\n",
            "Here's no more than where prison'd!\n",
            "Where-maid the mar?\n",
            "\n",
            "JULIET:\n",
            "In stirship; after him more is presenting\n",
            "In twifels rich, there was Claudio; a thought prove fit, office liveobly tale work of your boy in a gentleman.\n",
            "\n",
            "Nurse:\n",
            "Here, i'll hear men's wounds to thee,\n",
            "Of trump-to't nurse, ay no more. Is a resent,\n",
            "But hath and thy noblines woar'd,\n",
            "And wit sum now for a die supperites;\n",
            "he would have \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 100000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "torch.save(m.state_dict(), 'model.pth')\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "the_model = model.to(device)\n",
        "the_model.load_state_dict(torch.load('model.pth'))\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(the_model.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fmogr3i0KVj",
        "outputId": "26f3ddd8-19a1-4543-ba58-0e7f2f68176b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([58, 46, 47, 57,  1, 47, 57,  1, 39, 61, 43, 57, 53, 51, 43,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l6ayPbH0KII",
        "outputId": "76c439d4-0b8f-4ceb-fe3d-fc801946a21d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[58, 46, 47, 57,  1, 47, 57,  1, 39, 61, 43, 57, 53, 51, 43,  2]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "the_model = model.to(device)\n",
        "the_model.load_state_dict(torch.load('model.pth'))\n",
        "the_model.eval()\n",
        "# generate from the model\n",
        "input = \"this is awesome!\"\n",
        "context = encode(input)\n",
        "x = torch.tensor([context], dtype=torch.long)\n",
        "# ix = torch.randint(len(context) , (1,))\n",
        "# x = torch.stack([x[:]])\n",
        "# y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "context = x.to(device)\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(the_model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "0oWkmJpdq8qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context1 = torch.tensor(context, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5LLECw-ra2p",
        "outputId": "be328692-a4f6-40d6-fcf7-d08c53653c70"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-059ca6667bd4>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  context1 = torch.tensor(context, dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV30ZxFfvfk6",
        "outputId": "b93bd4b4-983f-42ce-e76f-867d39c6a276"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([58, 46, 47, 57,  1, 47, 57,  1, 39, 61, 43, 57, 53, 51, 43,  2],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFA-KMoev3zJ",
        "outputId": "7d4550f3-bfc5-4e8b-ec28-715a8582d33d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13,  4,  7,  8,  7,  2,  1,  8, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "dcRi9Ddo8Ikq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 100000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "# data = torch.tensor(encode(text), dtype=torch.long)\n",
        "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "# train_data = data[:n]\n",
        "# val_data = data[n:]\n",
        "\n",
        "# # data loading\n",
        "# def get_batch(split):\n",
        "#     # generate a small batch of data of inputs x and targets y\n",
        "#     data = train_data if split == 'train' else val_data\n",
        "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "#     x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "#     x, y = x.to(device), y.to(device)\n",
        "#     return x, y\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def estimate_loss():\n",
        "#     out = {}\n",
        "#     model.eval()\n",
        "#     for split in ['train', 'val']:\n",
        "#         losses = torch.zeros(eval_iters)\n",
        "#         for k in range(eval_iters):\n",
        "#             X, Y = get_batch(split)\n",
        "#             logits, loss = model(X, Y)\n",
        "#             losses[k] = loss.item()\n",
        "#         out[split] = losses.mean()\n",
        "#     model.train()\n",
        "#     return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "the_model = model.to(device)\n",
        "the_model.load_state_dict(torch.load('model.pth'))\n",
        "the_model.eval()\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    context = encode(text)\n",
        "    x = torch.tensor([context], dtype=torch.long)\n",
        "    context = x.to(device)\n",
        "    text = decode(the_model.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "    return text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=analyze_sentiment,\n",
        "    inputs=gr.Textbox(),\n",
        "    outputs=[\"text\"],\n",
        "    layout=\"vertical\",\n",
        "    title=\"text generation\",\n",
        "    description=\"Enter a text and i generate the rest.\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "DaBVps8czUtP",
        "outputId": "ebae7f4a-3f74-4e15-f80e-e6c5b28e0e5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-a064afa1593f>:200: GradioDeprecationWarning: `layout` parameter is deprecated, and it has no effect\n",
            "  iface = gr.Interface(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://d2b311441ff94cb446.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d2b311441ff94cb446.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d2b311441ff94cb446.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYeZEZBl8xl8",
        "outputId": "13e50d5e-317b-46be-964f-16d4dc259934"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio)\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.13)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=ac4926df354a55182d4d54e1a91fdfb9ae3d14632c2257b93a50f927a518c579\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.104.0 ffmpy-0.3.1 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 orjson-3.9.10 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 typing-extensions-4.8.0 uvicorn-0.23.2 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxQTg7sr3cHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}